# -*- coding: utf-8 -*-
"""tokenization (uni,bi,tri)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZFC1c6MB-DgQpsTUAXyKP3PpZ8eoISuI
"""

from nltk.tokenize import TweetTokenizer
tokenizer = TweetTokenizer()
text = "This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--"
tokenizer.tokenize(text)

# https://pypi.python.org/pypi/libarchive
!apt-get -qq install -y libarchive-dev && pip install -U libarchive
import libarchive
import nltk
from nltk.util import ngrams
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk import tokenize
# from nltk import word_tokenizer

text= " I am enjoying my life at UET"

token=nltk.word_tokenize(text)
unigram = ngrams(text.split(), n=1)
list(unigram)
bgs=nltk.bigrams(token)
tri=nltk.trigrams(token)
print(token)
print(list(bgs))
print(list(tri))
print(nltk.pos_tag(token))

import nltk
nltk.download('punkt')

text = "The best performance can bring in sky high success."
tokenizer = nltk.word_tokenize(text)  	

print(list(nltk.bigrams(tokenizer)))

import nltk
nltk.download('punkt')

text = "The best performance can bring in sky high success."
tokenizer = nltk.word_tokenize(text)  	

print(list(nltk.trigrams(tokenizer)))